<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- DELETE THIS SCRIPT if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-88470104-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-88470104-1');
  </script>

  <title>Abhinav Agarwalla</title>
  
  <meta name="author" content="Abhinav Agarwalla">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Abhinav Agarwalla</name>
              </p>
              <p>
		      I am a Software Engineer at Latitude AI, building computer vision models for self-driving vehicles.
              </p>

              <p>
                Previously, I was a Machine Learning Research Engineer at <a href="https://neuralmagic.com/">Neural Magic</a>, which recently got acquired by RedHat. Before this, I worked at <a href="https://argo.ai"> Argo AI</a> as a Research Engineer. Before that, I was a Masters student at the <a href="https://www.ri.cmu.edu/">Robotics Institute</a>, Carnegie Mellon University, where I worked with <a href="http://www.cs.cmu.edu/~deva/">Prof. Deva Ramanan</a>. Before that, I worked at <a href="http://val.serc.iisc.ernet.in/valweb/">Video Analytics Lab</a>, Indian Institute of Science, on computer vision problems and at Microsoft, improving search at Bing scale.
                I graduated from <a href="https://www.iitkgp.ac.in/">Indian Institute of Technology Kharagpur</a> majoring in Mathematics and Computing.
              </p>
              
              <p>
              I have enjoyed working on a diverse set of topic - ranging from video summarisation, motion planning for mobile robots to applying machine learning for pressing issues such as estimating insulin intake, detecting tumorous cells, etc. 
              
              <p style="text-align:center">
                <a href="mailto:agarwallaabhinav@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/AbhinavResume.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=d0rBsEAAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/abhinavagarwalla/"> LinkedIn </a> &nbsp/&nbsp
                <a href="https://github.com/abhinavagarwalla"> Github </a>
              </p>
		    
		          
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/photo_cropped.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/photo_cropped.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
	      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I am excited about developing state-of-the-art algorithms and formulating relevant research problems that enablerobots to sense and perceive the world as humans do. Presently, my research is focused on autonomous driving andpoint clouds, unsupervised learning, domain adaptation and transfer learning
              </p>
              <p> Reviewer: CVPR 2022-2024, ICRA 2021, 2023-2024, NeurIPS 2023 DGM4H, NeurIPS 2021 ICBINB, MLRC 2020, 2022. </p>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Papers</heading> (* joint first authors)
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:20px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <!-- <heading>Publications</heading> -->
          <tr>
            <td width="30%"><img src="https://mostlps.github.io/figures/teaser.png" alt="3DSP" width="250" height="100" style="border-style: none">
            <td valign="top" width="70%">
              <a href="https://arxiv.org/pdf/2310.12464">
                      <papertitle>Lidar Panoptic Segmentation and Tracking without Bells and Whistles</papertitle>
              </a>
            <br>
            <strong><u>Abhinav Agarwalla</u></strong><sup>*</sup>,
            Xuhua Huang<sup>*</sup>,
            Jason Ziglar,
            Francesco Ferroni,
            Laura Leal-Taixé,
            James Hays,
            Aljoša Ošep,
            Deva Ramanan
            <br>
              <em>IROS, 2023</em><br>
              <a href="https://arxiv.org/pdf/2310.12464">paper</a> /
              <a href="https://github.com/abhinavagarwalla/most-lps">code</a>
              <p align="justify">The paper introduces a detection-centric network for lidar panoptic segmentation (LPS) and tracking on 3D Lidar point clouds, challenging the conventional "bottom-up" approach. The network utilizes trajectory-level point supervision to obtain `modal` annotations. These are then utilized to predict fine-grained instance segments across time.</p> 
            </td>
          </tr>

	        <tr>
            <td width="30%"><img src="images/cssccnn_c.png" alt="3DSP" width="250" height="150" style="border-style: none">
            <td valign="top" width="70%">
              <a href="https://arxiv.org/abs/2009.06420">
                      <papertitle>Completely Self-Supervised Crowd Counting via Distribution Matching</papertitle>
              </a>
            <br>
            <strong><u>Abhinav Agarwalla</u></strong><sup>*</sup>,
            Deepak Babu Sam<sup>*</sup>,
            Jimmy Joseph,
            Vishwanath A. Sindagi,
            R. Venkatesh Babu,
            Vishal M. Patel
            <br>
              <em>ECCV, 2022</em><br>
              <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136910180.pdf">paper</a> /
              <a href="https://github.com/val-iisc/css-ccnn">code</a>
              <p align="justify">Existing self-supervised approaches can learn good representations, but require some labeled data to map these features to the end task of crowd density estimation. We mitigate this issue with the proposed paradigm of complete self-supervision, which does not need even a single labeled image.</p> 
            </td>
          </tr>
		
          <tr>  
            <td width="30%"><img src="images/beyondfeatures.png" alt="3DSP" width="250" height="150" style="border-style: none">
            <td valign="top" width="70%">
              <a href="">
                <papertitle>Beyond Learning Features: Training a Fully-functional Classi-fier with ZERO Instance-level Labels</papertitle>
              </a>
            <br>
            <strong><u>Abhinav Agarwalla</u></strong><sup>*</sup>,
            Deepak Babu Sam<sup>*</sup>,
            R. Venkatesh Babu
            <br>
              <em>AAAI, 2022</em><br>
              <em>ICML 2021 Workshop on Self-Supervised Learning for Reasoning and Perception</em><br>   
              <a href="https://icml21ssl.github.io/pages/files/paperid_23_truess_icmlsslworkshop_poster.pdf">poster</a> 
              <p align="justify">Existing unsupervised methods require some annotated samples to facilitate the final task-specific predictions.
                Instead, we leverage the distribution of labels for supervisory signal such that no image-label pair is needed for training a classifier.</p> 
            </td>
          </tr>

          <tr>
            <td width="30%"><img src="images/towardsdeployable.png" alt="3DSP" width="250" height="150" style="border-style: none">
            <td valign="top" width="70%">
              <a href="">
                      <papertitle>Towards Deployable Multi-Domain Learning for Inductive-Transductive Transfer</papertitle>
              </a>
            <br>
            <strong><u>Abhinav Agarwalla</u></strong><sup>*</sup>,
            Jogendra Nath Kundu<sup>*</sup>,
            Suvaansh Bhambri,
            Varun Jampani,
            R. Venkatesh Babu
            <br>
              <em>Technical Report</em><br>
              <a href="data/UMDN.pdf">paper</a>
              <p align="justify">We propose a universal framework to handle both task and domain shifts, and report state-of-the-art results on single/multi-source domain adaptation and domain generalisation.</p> 
            </td>
          </tr>
          
          <tr>
            <td width="30%"><img src="images/bayesopt_robo.png" alt="3DSP" width="250" height="150" style="border-style: none">
            <td valign="top" width="70%">
              <a href="https://dl.acm.org/doi/pdf/10.1145/3152494.3152502">
                      <papertitle>Bayesian optimisation with prior reuse for motion planning in robot soccer</papertitle>
              </a>
            <br>
            <strong><u>Abhinav Agarwalla</u></strong><sup>*</sup>,
            Arnav Kumar Jain<sup>*</sup>,
            KV Manohar,
            Arpit Tarang Saxena,
            Jayanta Mukhopadhyay
            <br>
              <em>CoDS - COMAD</em> 2018<br>
              <a href="https://dl.acm.org/doi/pdf/10.1145/3152494.3152502">paper</a>
              <p align="justify">We integrate learning and motion planning for soccer playing differential drive robots using Bayesian optimisation.  </p> 
            </td>
          </tr>
          
          <tr>
            <td width="30%"><img src="images/adversarial_da.png" alt="3DSP" width="250" height="150" style="border-style: none">
            <td valign="top" width="70%">
              <a href="https://dl.acm.org/doi/pdf/10.1145/3293353.3293423">
                      <papertitle>Unsupervised Domain Adaptation for Learning Eye Gaze from a Million Synthetic Images: An Adversarial Approach</papertitle>
              </a>
            <br>
            Avisek Lahiri<sup>*</sup>,
            <strong><u>Abhinav Agarwalla</u></strong><sup>*</sup>,
            Prabir Kumar Biswas
            <br>
              <em>ICVGIP </em> 2018<br>
              <a href="https://dl.acm.org/doi/pdf/10.1145/3293353.3293423">paper</a> /
              <a href="https://github.com/abhinavagarwalla/adversarial_da_icvgip18">code</a>
              <!-- <p align="justify"> </p>  -->
            </td>
          </tr>

          <tr>
            <td width="30%"><img src="images/rec_addr.png" alt="3DSP" width="250" height="150" style="border-style: none">
            <td valign="top" width="70%">
              <a href="https://arxiv.org/abs/1611.06492">
                      <papertitle>Recurrent Memory Addressing for describing videos</papertitle>
              </a>
            <br>
            Arnav Kumar Jain<sup>*</sup>,
            <strong><u>Abhinav Agarwalla</u></strong><sup>*</sup>,
            Kumar Krishna Agrawal,
            Pabitra Mitra
            <br>
              <em>DeepVision </em>, CVPR 2017<br>
              <a href="https://arxiv.org/abs/1611.06492">paper</a>
              <p align="justify">In this paper, we introduce Key-Value Memory Networks to a multimodal setting and a novel key-addressing mechanism to deal with sequence-to-sequence models. The proposed model naturally decomposes the problem of video captioning into vision and language segments, dealing with them as key-value pairs. </p> 
            </td>
          </tr>

          <tr>
            <td width="30%"><img src="images/ran_lstm.png" alt="3DSP" width="250" height="150" style="border-style: none">
            <td valign="top" width="70%">
              <a href="https://arxiv.org/pdf/1707.08814">
                      <papertitle>Representation-aggregation networks for segmentation of multi-gigapixel histology images</papertitle>
              </a>
            <br>
            <strong><u>Abhinav Agarwalla</u></strong><sup>*</sup>,
            Muhammad Shaban,
            Nasir M Rajpoot
            <br>
              <em>Deep Learning in Irregular Domains</em>, BMVC 2017<br>
              <a href="https://arxiv.org/pdf/1707.08814">paper</a> /
              <a href="https://github.com/abhinavagarwalla/Tissue_GUI">code</a>
              <p align="justify">CNNs have become the preferred choice for most computer vision tasks. However, these are not best suited for
                multi-gigapixel resolution Whole Slide Images (WSIs) of histology slides due to large
                size of these images. This work address this issue with novel 2D-LSTM + CNN network for tumor segmentation.</p> 
            </td>
          </tr>

        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
                  <hr>
                  <p align="center">
                  <a href="https://prithv1.xyz/">Fork of a fork :)</font>
                  </p>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
